<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[基于海洋学统计模型的海面波形生成]]></title>
    <url>%2F2018%2F05%2F18%2F%E5%9F%BA%E4%BA%8E%E6%B5%B7%E6%B4%8B%E5%AD%A6%E7%BB%9F%E8%AE%A1%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%B5%B7%E9%9D%A2%E6%B3%A2%E5%BD%A2%E7%94%9F%E6%88%90%2F</url>
    <content type="text"><![CDATA[BriefNV的流程图介绍该算法简直再清楚不过了： Phillips Spectrum统计模型的示例频谱是Phillips Spectrum： $$P_h(\vec k) = \left \langle { \left | {\widetilde h}^*(\vec k, t) \right | }^2 \right \rangle$$ 在统计学上使用如下模型来模拟Phillips Spectrum： $$P_h(\vec k) = \frac A { |\vec k|^4} { | \hat k \cdot \hat \omega | }^2 {e^{-\frac 1 { { |\vec k| }^2 L^2}}}$$ 生成Phillips Spectrum的像素着色器如下： 1234567891011121314151617181920212223#version 300 esprecision mediump float;in vec2 Position0;out vec2 Phillips;const float V = 1e4; // wind speed const vec2 u = normalize(vec2(1, -1)); // wind directionconst float g = 9.8; // gravityconst float L = V * V / g; // parameterconst float A = .02; // amplitudevoid main()&#123; vec2 uv = Position0; float k = length(uv); vec2 kn = normalize(uv); float d = dot(kn, u); vec2 ph = vec2(A / pow(k, 4.) * d * d * exp(-1./(k * k * L * L))); Phillips = sqrt(ph / 2.);&#125; Gaussian Random Numbers在原论文中表述为： $$\xi(\vec k) = \xi_r + i \xi_i$$ 其实部和虚部是两个独立的标准正态分布变量。 后续会写一下这部分。 Initial Spectrum$$\widetilde H_0(\vec k) = \frac 1 {\sqrt 2} \xi(\vec k) {\sqrt {P_h(\vec k)}}$$ 实际生成频谱如图： 现有一个疑惑：NV的频谱是不对称的，而我生成的频谱是对称的。单看这个函数表达式感觉没有不对称的理由，不知我理解是不是有偏差。 Displacement MapHeight field： $$\widetilde H(\vec k, t) = \widetilde H_0(\vec k) e^{i \omega t} + {\widetilde H_0}^*(- \vec k) e^{- i \omega t}$$ 即： $$\widetilde H(\vec k, t) = (cos(\omega t) + i sin(\omega t)) \widetilde H_0(\vec k) + (cos(\omega t) - i sin(\omega t)) {\widetilde H_0}^*(- \vec k)$$ Choppy field： $$\widetilde D_x(\vec k, t) = i \frac {\vec k.x} { | \vec k | } \widetilde H(\vec k, t)$$ $$\widetilde D_y(\vec k, t) = i \frac {\vec k.y} { | \vec k | } \widetilde H(\vec k, t)$$ 计算三个channel的像素着色器如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#version 300 es#define PI 3.141592654#define EPS .00precision mediump float;uniform sampler2D gSpectrum;uniform float gTime;in vec2 Position0;layout (location = 0) out vec2 H;layout (location = 1) out vec2 Dx;layout (location = 2) out vec2 Dy;const float omega = .5;vec2 complexMul(vec2 a, vec2 b)&#123; return vec2(a.x*b.x-a.y*b.y, a.x*b.y+a.y*b.x);&#125;float UVRandom(vec2 uv, float salt, float random)&#123; uv += vec2(salt, random); return fract(sin(dot(uv, vec2(12.9898, 78.233))) * 43758.5453);&#125;vec2 gaussian(vec2 uv, vec2 seed)&#123; float rand1 = UVRandom(uv, 10.612, seed.x); float rand2 = UVRandom(uv, 11.899, seed.y); float x = sqrt(2. * log(rand1 + 1.)); float y = 2. * PI * rand2; return x * vec2(cos(y), sin(y)); &#125;void main()&#123; float t = gTime; vec2 uv = Position0; float k = length(uv); float sinv = sin(omega * t); float cosv = cos(omega * t); vec2 tex = uv * .5 + .5; vec2 seed = vec2(1., 2.) * t * 1e-7; vec2 h0 = texture(gSpectrum, tex).xy; vec2 H0 = complexMul(h0, gaussian(tex, seed * .5 + 1.)); vec2 h0_conj = texture(gSpectrum, -tex).xy * vec2(1, -1); vec2 H0_conj = complexMul(h0_conj, gaussian(tex, seed)); H = (H0 + H0_conj).xy * cosv + (H0 - H0_conj).yx * vec2(-1, 1) * sinv; Dx = (uv.x / k * H).yx * vec2(-1., 1.); Dy = (uv.y / k * H).yx * vec2(-1., 1.);&#125; 结果： Height field Choopy field x Choppy field y 基本和NV的结果差不多。 Cooley–Tukey FFT - GPU Version这里我们假定本文的读者都了解并熟悉Cooley-Tuley FFT算法（可以去维基百科简单了解一下），主要说明如何将本算法用普通的GPU渲染管线实现。 普通渲染管线的特点： 纯并行，各线程间无法同步 每一个像素无法获取其他像素的任何信息 没有共享内存 普通渲染管线输入来源： uniform变量（对于同一个draw call而言是常量） 顶点着色器输出varyings（的线性插值） 纹理（可以有多个channel） 普通渲染管线的输出： 屏幕 RenderBuffer（开MRT后可以输出多个channel） 纹理（开MRT后可以输出多个channel） 要在普通渲染管线上做FFT显然不能采用FFT的递归版本（因为没有共享内存，无法存放中间结果）。观察Cooley-Tukey FFT的流程图，我们发现它的中间结果是分层的。对大小为$N$的序列，中间结果共有$log N+1$层（+1在第一次的bit-reversal copy），每层的大小完全相同（为$N$），每层的寻址方式基本相同（每段分两组寻址，对段长取模即可）。这可以非常自然地使用双纹理交替渲染解决。要对三个channel分别做FFT，只需开启MRT并对接三个channel即可。 二维FFT只要重复做两次一维FFT即可。 渲染流程： 123456789101112131415// pseudo codegenerateFFTData();bitReverseCopyHorizontal();swapBuffers();for (let i = 0; i != N; i *= 2) &#123; fftHorizontal(); swapBuffers();&#125;bitReverseCopyVertical();swapBuffers();for (let i = 0; i != N; i *= 2) &#123; fftVertical(); swapBuffers();&#125;generateDisplacement(); Bit-Reverse Copy 12345678910111213141516171819202122232425262728#version 300 esprecision mediump float;uniform sampler2D gPrevH;uniform sampler2D gPrevDx;uniform sampler2D gPrevDy;uniform float gN;layout (location = 0) out vec2 H;layout (location = 1) out vec2 Dx;layout (location = 2) out vec2 Dy;void main()&#123; vec2 mn = floor(gl_FragCoord.xy); float revx = 0.; for (int i = int(gN); i &gt; 1; i /= 2) &#123; revx = revx * 2. + mod(mn.x, 2.); mn.x = floor(mn.x / 2.); &#125; vec2 tex = vec2(revx, mn.y) / gN; H = texture(gPrevH, tex).rg; Dx = texture(gPrevDx, tex).rg; Dy = texture(gPrevDy, tex).rg;&#125; 一次迭代DFT： 12345678910111213141516171819202122232425262728293031323334#version 300 esprecision mediump float;#define PI 3.141592654uniform sampler2D gPrevH;uniform sampler2D gPrevDx;uniform sampler2D gPrevDy;uniform float gStep;uniform float gN;layout (location = 0) out vec2 H;layout (location = 1) out vec2 Dx;layout (location = 2) out vec2 Dy;vec2 complexMul(vec2 a, vec2 b)&#123; return vec2(a.x*b.x-a.y*b.y, a.x*b.y+a.y*b.x);&#125;void main()&#123; vec2 mn = floor(gl_FragCoord.xy); float k = mod(mn.x, gStep * 2.); float theta = PI * k / gStep; vec2 eo = vec2(mn.x, mn.x + gStep) - step(gStep, k) * gStep; vec2 epos = vec2(eo.x, mn.y) / gN; vec2 opos = vec2(eo.y, mn.y) / gN; vec2 term = vec2(cos(theta), sin(theta)); H = texture(gPrevH, epos).rg + complexMul(texture(gPrevH, opos).rg, term); Dx = texture(gPrevDx, epos).rg + complexMul(texture(gPrevDx, opos).rg, term); Dy = texture(gPrevDy, epos).rg + complexMul(texture(gPrevDy, opos).rg, term);&#125; 最后合并三个channel到displacement map即可： 123456789101112131415161718192021#version 300 esprecision mediump float;uniform sampler2D gPrevH;uniform sampler2D gPrevDx;uniform sampler2D gPrevDy;uniform float gN;layout (location = 0) out vec3 Displacement;void main()&#123; vec2 mn = floor(gl_FragCoord.xy); float term = 1. - 2. * mod(mn.x + mn.y, 2.); vec2 tex = mn / gN; float H = term * texture(gPrevH, tex).x; float Dx = term * texture(gPrevDx, tex).x; float Dy = term * texture(gPrevDy, tex).x; Displacement = vec3(Dx, Dy, H) * 3e-4;&#125; 最终生成的displacement map如图： 实际绘制的高度场如图： Normal直接从displacement map上取样计算法线。 $$\vec u_1 = \vec u_0 + \vec D_{i+1, j} - \vec D_{i, j}$$$$\vec v_1 = \vec v_0 + \vec D_{i, j+1} - \vec D_{i, j}$$$$\vec N = \vec u_1 \times \vec v_1$$ 其中$D$为displacement map，$u_0$ $v_0$为网格大小。 原文中有推导出法线公式，但若按此公式计算需要六趟FFT，有些吃不消。故退而求其次，选择了上述方法。 实际绘制的海面场景如图： Whitecap参照论文Real-time animation and rendering of ocean whitecaps中的方法，使用雅各比行列式求出水平运动对水面的撕扯程度： $$j(\vec p, t) = \left|\begin{array}{cccc} 1 + \sum_i \frac {\partial u(\vec p, t)} {\partial x} &amp; \sum_i \frac {\partial u(\vec p, t)} {\partial y} \\ \sum_i \frac {\partial u(\vec p, t)} {\partial y} &amp; 1 + \sum_i \frac {\partial v(\vec p, t)} {\partial y}\end{array}\right|$$ 水面激起白沫的颜色与$j(\vec p t)$假定为线性关系，调参至美观，得到： Bump map增加聊胜于无的细节，添加Bump map后的绘制结果： 调整色域、网格大小、修复模型后的绘制结果： Filtering要扩展到无穷大海面，除了LOD问题外，还要解决采样的周期性问题。NV的slides表明在远处可使用柏林噪声与FFT波形混合，此处用sigmod函数（$f(x) = \frac 1 {1 + e^{-x}}$）做了两次混合。 直接采样的法线，无Filter： 两次Sigmod Filter，FFT-&gt;perlin-&gt;$\hat z$： 最终的绘制结果基本令人满意。 Details texture的filter和wrap方式值得注意。如果设置的filter方式是LINEAR，则FFT采样时会拿到插值后的颜色值，导致FFT结果不正确。在实际海浪采样displacement map时，应选择warp方式为REPEAT，可以省去手动取模的麻烦。 GLSL ES3已经支持所有C语言的运算符（包括位运算）了。 Github repo 参考 Tessondorf - Simulating Ocean Water Nvidia - OceanCS slides GPU FFT Different FFT Algorithms High Performance Discrete Fourier Transforms on Graphics Processors Real-time animation and rendering of ocean whitecaps 使用快速傅里叶变换加速波形计算 实时水面模拟与渲染]]></content>
      <tags>
        <tag>GL</tag>
        <tag>WebGL</tag>
        <tag>wave-generating</tag>
        <tag>FFT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三天不重构就浑身难受]]></title>
    <url>%2F2018%2F05%2F16%2F%E4%B8%89%E5%A4%A9%E4%B8%8D%E9%87%8D%E6%9E%84%E5%B0%B1%E6%B5%91%E8%BA%AB%E9%9A%BE%E5%8F%97%2F</url>
    <content type="text"><![CDATA[我又重构我的小引擎了！ 这些技术终于来了，令我觉得相见恨晚。 可能是我太弱智了（ Floating Point Texture听说你想要一个浮点纹理？我这里有啊！ 在webgl1.0中作为扩展的OES_texture_float在webgl2.0中已经转正啦！ 在webgl1.0中：12gl.getExtension("OES_texture_float"); // 使用浮点纹理gl.getExtension("OES_texture_float_linear"); // 使用浮点纹理的linear filter 在webgl2.0中：12gl2.getExtension("EXT_color_buffer_float"); // 渲染到浮点纹理gl2.getExtension("OES_texture_float_linear"); // 使用浮点纹理的linear filter 遇到的坑： webgl2.0原生支持浮点纹理，但是默认不允许渲染到浮点纹理（虽然大多数浏览器都支持），需要手动require一个扩展EXT_color_buffer_float。 在webgl2.0中，设置浮点纹理的internal format和webgl1.0中的处理办法可能不太相同。如：1234// webgl 2.0gl2.texImage2D(gl.TEXTRUE_2D, 0, gl2.RGBA32F, width, height, 0, gl.RGBA, gl.FLOAT, null); // 注意 gl2.RGBA32F 和 gl.RGBA，第一个是 internal format// webgl 1.0 + OES_texture_floatgl.texImage2D(gl.TEXTRUE_2D, 0, gl.RGBA, width, height, 0, gl.RGBA, gl.FLOAT, null); // 不需要指定 internal format 参考： webgl 2.0 纹理数据类型一览 再也不用编码解码啦！ Multiple Render Targets听说你想一个pass渲染到多个纹理？用MRT吧！ 加了高模之后vertex shader需要的时间飞速增长。为了获取多种像素信息（法线、深度、高度、类型等），开多个pass渲染同一个场景显然是不明智的，这个时候MRT就派上用场了。 MRT允许你为像素着色器创建多个color attachment，在webgl2.0中，这些channel应为gl2.COLOR_ATTACHMENT0到gl2.COLOR_ATTACHMENT15。 流程： 绑定目标纹理到特定的attachment 设置当前激活的drawBuffers 在像素着色器中输出到attachment相对应的channel 恢复原先状态 使用MRT合并多个pass示例：123456789101112131415161718// set channels using gl.framebufferTexture2Dthis.offscreen.set(gl2.COLOR_ATTACHMENT0, this.mainImage);this.offscreen.set(gl2.COLOR_ATTACHMENT1, this.normalDepthImage);this.offscreen.set(gl2.COLOR_ATTACHMENT2, this.typeImage);// specify active channels list, order does not mattergl2.drawBuffers([ gl2.COLOR_ATTACHMENT0, gl2.COLOR_ATTACHMENT1, gl2.COLOR_ATTACHMENT2]);gl.clear(gl.COLOR_BUFFER_BIT | gl.DEPTH_BUFFER_BIT);// render your targets with specified fragment shader.gl2.drawBuffers([ gl2.COLOR_ATTACHMENT0]); 像素着色器：123456789101112131415161718#version 300 esprecision mediump float;// location corresponds to attachment idlayout (location = 0) out vec4 FragColor;layout (location = 1) out vec4 NormalDepth;layout (location = 2) out float Type;void main()&#123; // process data.. // output to three channels (corresponds to textures) FragColor = // ... NormalDepth = // ... Type = // ...&#125; Transform Feedback虽然有人认为这是webgl2.0的一个鸡肋功能（因为没有几何着色器），但我相信这玩意还是能有它自己的用途的。 TransformFeedback说白了就是这样一种功能：它可以捕获顶点着色器的输出变量（varyings），并且把它们的值存储到buffer里，这样你就不用在CPU上更新顶点数据并且频繁gl.bufferData了。 webgl不能输出到当前的输入buffer，这意味着我们需要使用双buffer。在bufferA中存储所有图元，然后渲染到bufferB，然后swap buffers。因为webgl2.0没有几何着色器，所以输出图元的数量和输入图元的数量应当是一样的，只要保证输出buffer的layout和输入buffer相同，我们就可以轻易地创建一个循环来完成所有的transform工作。 另外，TransformFeedback不能在光栅化前终止（意味着它不能不渲染到一个目标），所以片元着色器依然是必须的，这样可以认为它是需要结合离屏渲染使用的。 我目前想到的使用场景是计算 物体轨迹。（拉烟什么的） 后续会对这一部分进行深入研究。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566class TBO &#123; private handle1; private handle0; constructor() &#123; if (!gl2) &#123; throw "webgl 2.0 required."; &#125; this.handle0 = gl2.createBuffer(); this.handle1 = gl2.createBuffer(); &#125; bind() &#123; gl.bindBuffer(gl.ARRAY_BUFFER, this.handle0); gl2.bindBufferBase(gl2.TRANSFORM_FEEDBACK_BUFFER, 0, this.handle1); &#125; unbind() &#123; gl2.bindBufferBase(gl2.TRANSFORM_FEEDBACK_BUFFER, 0, null); gl.bindBuffer(gl.ARRAY_BUFFER, null); &#125; data(data: any[]) &#123; gl.bufferData(gl.ARRAY_BUFFER, 4 * data.length, gl2.DYNAMIC_COPY); this.unbind(); this.swap(); this.bind(); gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(data), gl2.DYNAMIC_COPY); &#125; swap() &#123; let h = this.handle1; this.handle1 = this.handle0; this.handle0 = h; &#125; get handle(): any &#123; return this.handle0; &#125;&#125;class TransformFeedback &#123; public readonly handle: WebGLTransformFeedback; private vbo = new TBO(); constructor() &#123; if (!gl2) &#123; throw "webgl 2.0 required."; &#125; this.handle = gl2.createTransformFeedback(); &#125; bind() &#123; gl2.bindTransformFeedback(gl2.TRANSFORM_FEEDBACK, this.handle); &#125; unbind() &#123; gl2.bindTransformFeedback(gl2.TRANSFORM_FEEDBACK, null); &#125;&#125;class TAO &#123; // ... draw(first: number = undefined, count: number = undefined) &#123; gl2.beginTransformFeedback(gl.POINTS); if (first != undefined) &#123; gl.drawArrays(gl.POINTS, first, count); &#125; else &#123; gl.drawArrays(gl.POINTS, 0, this.numVertices); &#125; gl2.endTransformFeedback(); &#125; // ...&#125; 最后，成功让一个点在屏幕上动了起来，并且可以改变颜色。 参考： WebGL 2.0 Specification Derivatives一个方法，可以让你对插值后的像素着色器输入求关于x和y的偏导数。 实际微分的值是通过获取相邻像素的对应varying并做差分得到的近似值。 想到的实际用途：如果像素着色器输入一个高度场，则可以通过refined point的差分的叉积确定当前像素的法向量：normalize(cross(dx, dy))，这种方法不需要事先对高度场求偏导，至少可以节省一部分顶点着色器的开销（尤其是对于一些non-trival的高度场尤其适用）。 参考： 差分的硬件实现 Khronos官方介绍 Some other TechniquesCaustics参考一个接近完美的焦散demo，写出了一个很挫的效果。不说了，丢人。 基本想法是用一个mesh来模拟光线的集合，并且把折射前后的两个三角形面积作比，得到当前区域内的平均光照（焦散强度）。 别人的demo： 我的丢人demo： 参考： 在线demo 实现原理 Fresnel Blending我可能之前把公式拟合错了，某几个参数是不对的，后来手动调参数拟合了一下，发现基本对上了。 渲染结果： 参考： GLSLで環境マップ＆フレネル効果 Geogebra Filtering根据物体类型降噪，做了一些，现在好多了。 OtherVisual Studio Code Live Share这功能简直太好用了，强推！ 换了新桌面 换了新主题 没能在5月14日发表 >_&lt;参考 知乎：最值得期待的WebGL 2.0功能]]></content>
      <tags>
        <tag>GL</tag>
        <tag>offscreen</tag>
        <tag>WebGL</tag>
        <tag>technique</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Screen Space Reflection]]></title>
    <url>%2F2018%2F05%2F03%2FScreen-Space-Reflection%2F</url>
    <content type="text"><![CDATA[前言最近在参考SH3的游戏截图的时候，发现船只在水里是有倒影的。 个人很喜欢这种效果，于是决定着手开始做。通过查阅相关资料，找到了对应的technique———屏幕空间反射（SSR）。 Post processSSR是一种后处理效果。所谓后处理，指的一般是先离屏将场景render到texture里，再用一个独立pass来加工texture。这时原先的buffer就可以通过一个sampler2D中获取。 既然画面和后处理不在同一趟pass，那么后处理时必然会缺失一些原场景中的信息（比如图元类型、顶点、法线、深度、位置等信息）。我们无法在后处理阶段获取任何原场景中的顶点信息（因为没有相应的存储结构），但是对于法线、深度、位置等信息，可以单独开一个pass将其写入texture中，这样在后处理阶段，每个像素对应的相应信息又可通过一个sampler2D来获取。 fragment attribute encodingopengl支持深度纹理、同时支持unsigned char、float、short、int等多种类型的纹理。而webgl 1.0似乎只支持unsigned char纹理，component也只有可怜的RGB、RGB565、RGBA、RGB5A1等几种。。。所以要将顶点信息映射到纹理坐标，首先要对其进行编码。 vec3一个三维向量，比如法线、颜色、位置等。 首先将该向量单位化（位置等跟模长有关的不能单位化，所以不能这么存），因为FragColor输出的rgb三个分量会被clamp到$[0, 1]$之间。要保证xyz三个坐标都在$[0, 1]$之间，需要再将单位化后的向量做变换：$X’ = 0.5 X + 0.5$。 解码用$X = 2 X’ - 1$即可。 float一个32bit的浮点数（比如深度）。 通过计算机组成，我们知道IEEE754的32位浮点数形如1-8-23这样，31位是符号位，23-30位是阶码。显然这个浮点数是不能直接通过FragColor的一个分量输出的。如上所述，FragColor会被clamp到$[0, 1]$之间（去掉阶码和符号位），并且被trunc掉尾数的后15位。 这样也很容易想到一个简单的浮点数编码方法（这里只针对单位化的浮点数），即直接去掉阶码和符号位，然后将尾数分段移位，保证每段有8个bit。这样做只需要23/8=3个float就可存储encode一个浮点数。这其实就是把浮点数当成定点数处理了，代码也很好写，只有几行： 12345678910111213141516// encodevec4 normfloat2vec4(float x)&#123; const vec4 bitShift = vec4(1.0, 256.0, 256.0 * 256.0, 256.0 * 256.0 * 256.0); const vec4 bitMask = vec4(1.0/256.0, 1.0/256.0, 1.0/256.0, 0.0); vec4 rgba = fract(x * bitShift); rgba -= rgba.gbaa * bitMask; return rgba;&#125;// decodefloat vec42normfloat(vec4 v)&#123; const vec4 bitShift = vec4(1.0, 1.0/256.0, 1.0/(256.0*256.0), 1.0/(256.0*256.0*256.0)); return dot(v, bitShift);&#125; compressed encoding对于一个场景而言，我们可能只需要它的深度信息和法线信息，这时用两个pass渲染场景可能有点浪费，如果场景中物体多了可能会吃不消。这时我们需要一种compressed method来编码发线和深度。 如上所述，如果我们用3个float来编码一个深度信息（透视投影的深度值总是单位化的），这是无损的。如果我们少用一个float，则我们会丢失深度值的最后7个bit，这在某些情况下是可以接受的。同理，我们可以将RGB32的法向量改用RGB565编码，这样会少用掉一个float。将这两个vec2合在一起，我们就可以在一个RGBA32（vec4）的纹理里同时存储法线和深度信息。 pros： 占用内存少 少一趟pass（对物体较多的场景比较适合） cons： 丢失精度 编码解码稍微复杂点 1234567891011121314151617181920212223242526272829303132333435363738394041424344// encodevec2 normfloat2vec2(float x)&#123; const vec2 bitShift = vec2(1.0, 256.0); const vec2 bitMask = vec2(1.0/256.0, 0.0); vec2 rg = fract(x * bitShift); rg -= rg.gg * bitMask; return rg;&#125;vec2 rgb2rgb565(vec3 rgb) &#123; float g1 = rgb.g / 32.; float g2 = fract(rgb.g * 8.); g2 -= fract(g2 * 8.) / 8.; return vec2(rgb.r - fract(rgb.r * 32.) / 32. + g1, g2 + rgb.b/8.);&#125;// FragColor = vec4(// rgb2rgb565(normalize((gV * vec4(Normal0, 0)).xyz) * .5 + .5), // normfloat2vec2(gl_FragCoord.z)// );// decodefloat vec22normfloat(vec2 v)&#123; const vec2 bitShift = vec2(1.0, 1.0/256.0); return dot(v, bitShift);&#125;vec3 rgb5652rgb(vec2 rgb565)&#123; float rr = fract(rgb565.r * 32.); float bb = fract(rgb565.g * 8.); return vec3(rgb565.r - rr / 32., rr + rgb565.g / 8. - bb / 64., bb);&#125;PointInfo decodePoint(vec2 uv)&#123; PointInfo info; vec4 tex = texture(gNormalDepth, (uv + 1.) * .5); info.normal = rgb5652rgb(tex.rg) * 2. - 1.; info.depth = vec22normfloat(tex.ba); return info;&#125; 我没有使用这种方法，原因有二： 场景比较空旷 对丢失景深精度比较敏感 Screen Space ReflectionReflection下面的所有向量都是在相机空间中的，因为在后处理阶段谈论世界空间一般来说是没有意义的。 为了计算fragment对应的反射向量，我们需要两个向量： 入射向量$I$ 法向量$N$ 法向量可以直接从uv对应的sampler2D中获取，这里主要讲入射向量。 入射向量是一个从相机到render point的向量（我们使用反向追踪）。相机在相机空间中的坐标是$[0, 0, 0, 1]^T$，故只需要算出当前render point在相机空间中的坐标即可。 我们知道，render point的uv坐标是在透视空间（屏幕空间）中的。要获取在相机空间中的坐标，乘一个透视矩阵的逆矩阵即可。glsl不具有求逆的功能，需要我们在cpu中先算好逆，再用uniform传进去。事实上，我们完全可以手动算出逆，然后直接hard code进shader里。 linearlizeDepth将透视空间中的z值refine回相机空间z值，事实上这个函数不是数学上正确的，但是可以得到近似的结果。 12345678910float linearlizeDepth(vec2 uv)&#123; return - gP[3].z / (1. - vec42normfloat(texture(gDepth, (uv + 1.) * .5)));&#125;vec3 refinePoint(vec2 uv)&#123; float z = linearlizeDepth(uv); return vec3(uv.x / gP[0].x, uv.y / gP[1].y, -1.) * z;&#125; 有了入射向量和法向量，我们就可以算出相机空间中的反射向量$R$。 Raymarching朴素的Raymarching想法是这样的：先refine当前position$P$，每次迭代加上一个$step \times R$，然后每次把新的$P$用透视矩阵投影到屏幕上，然后取depth。这样就要在每个iteration里做一次projection，耗费大概会比较大。 Depth based 2D Raymarching我们可以先把反射向量$R$做一次投影，算出它在屏幕空间的uv，然后只需要每次迭代加uv，并refine depth即可。 这种方法要注意的一个点是：屏幕空间不是一个线性空间（屏幕空间在乘了透视矩阵之后还将w分量归一化了，这里就非线性了）。对于点$[x, y, z, 1]^T$，在线性空间$W$中的对应点为$W \times [x, y, z, 1]^T$，所以对应的向量$[x, y, z, 0]^T$在$W$中的对应向量为$W \times [x, y, z, 0]^T$（$W \times [x_2, y_2, z_2, 1]^T - W \times [x_1, y_1, z_1, 1]^T = W \times [x, y, z, 0]^T$）。在透视空间$P$中，点$[x, y, z, 1]^T$对应的点为$W \times [x, y, z, 1]^T / z$，相应的向量$V = [x, y, z, 0]^T$也就不满足$V’ = P \times V / z$。实际上变换后的向量应该跟它的出发点有关，即得到屏幕空间中正确向量的做法是$V’ = P \times (O + V) / z_2 - P \times O / z_1$。 同样，透视空间中的深度也是非线性的，需要先做一遍refinement。 Enhancement - Binary search使用Binary search能保证用较少的迭代次数快速收敛到比较精确的深度。大概想法是在开始设置一个较大的步长，每次检测到碰撞（深度差值flip）后撤回上一步搜索并且减半步长。实际证明这种方法是非常有效的。 noise现在没做，以后再说。 会先看看Perlin noise等，之后再做。 Gallery相机空间法向量 透视空间深度 相机空间坐标 565法向量16深度纹理 屏幕空间反射 屏幕空间反射+水波 另外请自行想象一下debug shader是一种多么痛苦的过程。]]></content>
      <tags>
        <tag>GL</tag>
        <tag>water-rendering</tag>
        <tag>raymarching</tag>
        <tag>WebGL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018.Apr]]></title>
    <url>%2F2018%2F05%2F01%2F2018-Apr%2F</url>
    <content type="text"><![CDATA[最近我对rendering的痴迷程度达到了吃饭、上厕所和睡觉的时候都无法阻止去想的程度了。 CG换题和重构前一个拟题（巫师棋）被我废弃了。理由是对照老师的给分点，发现偏差比较大。最终题目定在了silent hunter的复刻版。 这可能是我最喜欢的一个题目，虽然当时还我不确定能不能做好，但是总要有一个破釜沉舟的决策出现。 对原C++17引擎用Tyoescript重构，只花了4天时间就超过了原有效果。 现在看了一眼整体的技术路线，除了烟雾、火光的粒子系统实现（可能需要抛弃所有webgl 1.0浏览器的支持）之外，所有技术路线都已经确定并可以实施。 实现了很多原有技术： Phong Lighting（已废弃，后续版本在渲染夜晚时可能加上） Shadow Cubemap（PCF不知道为什么崩了，现在去掉了阴影） Gerstner Wave（后续会被基于统计学模型的FFT替换） LOD（不一样的照搬，减少了DrawCall次数） 加入了很多新技术（很多是基于render screen square的）： Offscreen Rendering（Basis） Dynamic Ambient Cubemap（这个简直太棒了） Observer Viewport Fresnel Effect（目前处于崩溃状态） Defered Image 主要是重构了offscreen和texture的逻辑，使用起来达到了一种惊人的方便程度。终于可以放心地实现各种technique啦！ 后续马上要做的： Ubisoft Model Refinement FFT Raymarch based SSR（Screen Space Reflection） 下面是我的喃喃自语.. 数据库上半学期结束期中大程结束了。这裸js糊出来的工程我都不愿意多看一眼，交给助教验收的时候居然说我做的很好，WTF。 果然人类都是视觉动物啊。 动漫绘画结束希望老师不会因为我不是树莓的就拉低我的分（ 课程本身我还是很满意的，很休闲，最后几节课还能上板，很开心（ 计组计组有种要吃大礼包的感觉，平时得多加把劲了（ 职规这鬼课程终于没了（ 面向信息技术的沟通技巧我喜欢上陈越的课！ 夏学期突击计划，启动！整个周三和周二晚上都空出来了，一大块连续时间最适合做些莫名其妙的研究了.. ASC过了ASC之后可能会考虑退队，因为我已经不想再做一个既没有贡献又没有很大兴趣可言的队内寄生虫了。 然而我还是想和大家一起开心地玩啊>_&lt; 总结四月的时间（我醒着的时间）有三分之一在研究rendering，有三分之一在写代码，有三分之一在上课和写作业。娱乐时间基本没有，OSU和炉石都很少上了。总之，我个人还是很满意的（虽然还是亏欠了一些债务）。 希望自己能加把劲，继续努力。]]></content>
      <tags>
        <tag>thoughts</tag>
        <tag>GL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于迭代器失效]]></title>
    <url>%2F2018%2F04%2F08%2F%E5%85%B3%E4%BA%8E%E8%BF%AD%E4%BB%A3%E5%99%A8%E5%A4%B1%E6%95%88%2F</url>
    <content type="text"><![CDATA[::std::move容器并不会使迭代器失效，除非move改变了allocator。 123456789101112131415161718struct list_wrapper&#123; std::vector&lt;int&gt; m_list;&#125;;int main()&#123; std::vector&lt;int&gt; myList &#123; 1, 1, 2, 3, 5 &#125;; const std::vector&lt;int&gt;::iterator iter = myList.begin(); list_wrapper wrappedList; wrappedList.m_list = std::move(myList); // iter is still valid and points to the begin of m_list return 0;&#125; StackOverflow大法好]]></content>
      <tags>
        <tag>STL</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Object Picking]]></title>
    <url>%2F2018%2F03%2F30%2FObject-Picking%2F</url>
    <content type="text"><![CDATA[终于忙完了一摞奇怪的事，回来研究Object Picking了… 目前了解到的Object Picking方法有以下两种： Ray Picking FBO Picking 顾名思义，Ray Picking按射线方向对各物体判交，因为之前没有做几何检测（甚至AABB包围盒都没做），所以先pass掉了（SRTP对接VR手柄时这个是一定要做的）。另外一种方法（FBO Picking）比较有趣，此处做下记录。 FBO Picking Basic为场景中的物体各选一个不同的颜色，做一趟离线渲染，取出帧缓存鼠标位置的颜色，即获得Picking Object. ModifiedColor Buffer只用一个分量，把物体编号cast进去即可。 Improved创建一个鼠标指向的摄像机，缩小视角，渲染到一个1x1的矩阵即可。 遇到的坑glRenderbufferStorage的第二个参数要指定位宽和类型，把GL_R32F写成GL_RED，调了一年。 后记code 接下来准备研究如何让给物体添加外轮廓，大概是stencil buffer + normal displacement这样。 使用X1C是一种享受。]]></content>
      <tags>
        <tag>GL</tag>
        <tag>offscreen</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018.March]]></title>
    <url>%2F2018%2F03%2F28%2F2018-March%2F</url>
    <content type="text"><![CDATA[最近心情很好，一直很开心。 逐渐感觉发生什么事都无所谓了。 都是我的错。 另外：换了X1C 2018。]]></content>
      <tags>
        <tag>thoughts</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简要记录一下最近的情况]]></title>
    <url>%2F2018%2F03%2F18%2F%E7%AE%80%E8%A6%81%E8%AE%B0%E5%BD%95%E4%B8%80%E4%B8%8B%E6%9C%80%E8%BF%91%E7%9A%84%E6%83%85%E5%86%B5%2F</url>
    <content type="text"><![CDATA[简要记录一下最近的情况吧。 开学阶段总是比较杂乱> &lt; SRTP定题基于VR的多人合作3D建模系统。 有点头大，主要是精度问题。不过考虑到双手柄且手柄输入比鼠标多一个维度，感觉还具有一定的可实施性。服务器的问题倒是好办。 基本打算是这学期把VR部分基本搞出来，下学期上计网的同时去弄服务器，可以方便一些。 修改了Phong光照模型emm，其实很早就实现了，但是刚刚参考了Assimp和3Dmax光照部分相关参数，对原Phong模型进行了一些修正（原模型主要参考ogldev）。例如原Phong模型对specular的处理是： 12Light = Ambient + Shadow * (Specular + Diffuse);FragColor = Color * Light; 这样当物体为纯黑表面时FragColor总为零。实际上，镜面反射和物体本身的颜色是无关的，将光照模型修改如下： 123Light = Ambient + Shadow * Diffuse;LightSpec = Shadow * Specular;FragColor = Color * Light + LightSpec; 用上述方法渲染得到的结果如下： 近期TODO SRTP草案，PPT SQTP相关事宜，外加两篇文章 ADS：准备抢接下来的Project 计组：MIPS汇编器/反汇编器 实现抗锯齿，在Compute Shader中计算鼠标位置，实现骨骼动画。 SQL：期中大程，期末大程，权衡用QT还是Java（]]></content>
      <tags>
        <tag>thoughts</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[水面LOD相关]]></title>
    <url>%2F2018%2F02%2F27%2F%E6%B0%B4%E9%9D%A2LOD%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[看了GDC上Naughty Dog关于水体渲染的介绍，准备写点LOD相关内容。 Level of Details由于透视，近处的物体要比远处物体的细节丰富很多。在实时渲染时，如果一视同仁地渲染远处和近处的物体，不仅浪费了远处物体的细节，还降低了渲染速度，因此产生了LOD技术。 水面网格LOD以摄像机为中心生成一张正方形环状网格图，从内环到外环网眼大小逐渐增大，如图： 若要渲染尺寸为$W$，最小网格宽度为$e$的水面，不使用LOD的空间复杂度是$O(\frac {W^2} {e^2})$，使用LOD的空间复杂度为$O(\frac {W {W_0} + {W_0}^2 } {e^2})$，其中$W_0$为一级精细度渲染尺寸。 边界修复由于矩形环相接处边缘点没有啮合，这样渲染时会有缝隙，需要手动将边缘点与下一层对应点连接。如图： 此处渲染可用GL_TRIANGLE_FAN。 缺点如果远处波浪频率过大，会较严重地受到受网眼大小影响，在摄像机移动时在环面相交处会有闪烁现象。解决方案是忽略远处的高频波浪以及使用法线贴图。 后记 背景换成了暖色&gt;w&lt; 要开学了，又是令人激动的新学期呢（]]></content>
      <tags>
        <tag>GL</tag>
        <tag>LOD</tag>
        <tag>mesh-generating</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Gerstner Wave的水面shader]]></title>
    <url>%2F2018%2F02%2F22%2F%E5%9F%BA%E4%BA%8EGerstner-Wave%E7%9A%84%E6%B0%B4%E9%9D%A2shader%2F</url>
    <content type="text"><![CDATA[Gerstner Wave$$\left\{\begin{aligned}\begin{array}{rl}x &amp;= p - QDA\sin(D \cdot p - \omega t)\\y &amp;= A\cos(D \cdot p - \omega t)\end{array}\end{aligned}\right.$$ 参数曲面的法向量$$\left\{\begin{aligned}\begin{array}{rl}x &amp;= x(u, v)\\y &amp;= y(u, v)\\z &amp;= z(u, v)\end{array}\end{aligned}\right.$$固定$v$，得到关于$u$的曲线，在$p = (u, v)$处的切向量为$$t_u = \{\frac {\partial x} {\partial u}, \frac {\partial y} {\partial u}, \frac {\partial z} {\partial u} \}$$固定$u$，得到关于$v$的曲线，在$p = (u, v)$处的切向量为$$t_v = \{\frac {\partial x} {\partial v}, \frac {\partial y} {\partial v}, \frac {\partial z} {\partial v} \}$$曲面的法向量$n_p$同时与$t_u$，$t_v$垂直，故曲面在点$p = (u, v)$处的法向量为$$t_u \times t_v = \left|\begin{array}{cccc} i &amp; j &amp; k \\ x_u &amp; y_u &amp; z_u\\ x_v &amp; y_v &amp; z_v\end{array}\right|$$ 推导$$\left\{\begin{aligned}\begin{array}{rl}x &amp;= u - QD_xA\sin\alpha\\y &amp;= v - QD_yA\sin\alpha\\z &amp;= A\cos\alpha\end{array}\end{aligned}\right.$$其中$$\alpha = fD \cdot p - \omega t = f(uD_x + vD_y) - \omega t$$ $$\left\{\begin{aligned}\begin{array}{rl}t_u &amp;= \{1 - fQD_x^2A\cos\alpha, -fQD_xD_yA\cos\alpha, -fAD_x\sin\alpha\}\\t_v &amp;= \{- fQD_xD_yA\cos\alpha, 1 - fQD_y^2A\cos\alpha, -fAD_y\sin\alpha\}\end{array}\end{aligned}\right.$$法向量为$$n_p = \{fAD_x\sin\alpha, fAD_y\sin\alpha, 1 - fQA\cos\alpha\}$$ 对于$n$个Gerstner Wave叠加的情况，顶点为：$$\left\{\begin{aligned}\begin{array}{rl}x &amp;= u - \sum Q{D_x}A\sin\alpha\\y &amp;= v - \sum Q{D_y}A\sin\alpha\\z &amp;= \sum A\cos\alpha\end{array}\end{aligned}\right.$$法向量为：$$\left\{\begin{aligned}\begin{array}{rl}x &amp;= \sum fAD_x\sin\alpha\\y &amp;= \sum fAD_y\sin\alpha\\z &amp;= 1 - \sum fQA\cos\alpha\\end{array}\end{aligned}\right.$$ Vertex Shader12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#version 330 corelayout (location = 0) in vec3 Position;layout (location = 2) in vec3 Normal;layout (location = 5) in vec2 TexCoord;out vec2 TexCoord0;out vec3 Normal0;out vec3 WorldPos0;out vec3 Offset0;// out vec4 Color0;#define MAX_GERSTNER_WAVE_COUNT 128struct GerstnerWave &#123; float Steepness; float Amplitude; float Frequency; float Speed; vec2 Direction;&#125;;uniform GerstnerWave gGerstnerWave[MAX_GERSTNER_WAVE_COUNT];uniform int gGerstnerWaveCount;uniform float gTime;uniform mat4 gWVP;uniform mat4 gWorld;void GerstnerLevelOne(vec3 vertex, out vec3 offset, out vec3 normal)&#123; offset = vec3(0, 0, 0); normal = vec3(0, 0, 1); for(int i = 0; i &lt; gGerstnerWaveCount; i++) &#123; float dp = dot(vertex.xy, gGerstnerWave[i].Direction.xy); float p = gGerstnerWave[i].Frequency * dp - gGerstnerWave[i].Speed * gTime; float Acos = gGerstnerWave[i].Amplitude * cos(p); float Asin = gGerstnerWave[i].Amplitude * sin(p); offset.x -= gGerstnerWave[i].Steepness * gGerstnerWave[i].Direction.x * Asin; offset.y -= gGerstnerWave[i].Steepness * gGerstnerWave[i].Direction.y * Asin; offset.z += Acos; normal.x += gGerstnerWave[i].Frequency * gGerstnerWave[i].Direction.x * Asin; normal.y += gGerstnerWave[i].Frequency * gGerstnerWave[i].Direction.y * Asin; normal.z -= gGerstnerWave[i].Frequency * gGerstnerWave[i].Steepness * Acos; &#125; normal = normalize(normal);&#125;void main()&#123; vec3 offset; vec3 normal; vec3 Position0 = Position; GerstnerLevelOne(Position0, offset, normal); Position0 += offset; gl_Position = gWVP * vec4(Position0, 1.0); // TexCoord0 = TexCoord; if (normal.z &lt; 0) normal.z = -normal.z; Normal0 = (gWorld * vec4(normal, 0.0)).xyz; WorldPos0 = (gWorld * vec4(Position0, 1.0)).xyz; // Color0 = vec4(clamp(Position0, 0.0, 1.0), 1.0); // gl_Position = gWVP * vec4(Position, 1.0); Offset0 = offset;&#125; 效果图]]></content>
      <tags>
        <tag>GL</tag>
        <tag>water-rendering</tag>
        <tag>wave-generating</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenGL学习]]></title>
    <url>%2F2018%2F02%2F16%2FOpenGL%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[OpenGL最近为学习下学期的计图做准备。刚刚读完《一步步学习OpenGL3.3+》，并阅读了Zurl先辈的游戏引擎ElfGE源码。在学习过程中将OpenGL进行了基本封装，加上Assimp，摄像机和灯光，构建了一个基本的3D绘制框架bee，并实现了一些可以渲染3D模型的实例。 渲染结果看上去是这样的： 期间又学习了一些3D建模工具的使用（3Dmax，3DBuilder for win10），bismark是我在SH3的模型数据里偷的（ 基本的图形学知识已经有了，接下来准备学习水面渲染。 水面渲染初步目前水面渲染一般采用如下方法： 将水面视为一张矩形2D网格，并将每个矩形网眼分为上下两个三角形，确定网格各点的2D坐标。 对每帧，计算网格各点的高度坐标和顶点法线。 其中步骤1可直接在初始化过程计算，步骤二则需要大量计算，一般使用vertex shader完成。 正弦波叠加略去… 平静水面的法向量模拟法对于平静的水面，我们可以只计算顶点的法向量，而将z值置为0。这样水面只有在计算反射光时会显现出波纹，计算量较小。 Gerstner Wave在计算机图形学中，Gerstener Wave是一种应用广泛的水面模拟方法。 离线波形生成对于给定参数方程的模拟波形，可以离线生成一个周期的波形，运行时通过顶点的位置来确定相位，查表获取z值。由于Gersterner Wave具有对称性，只需生成半个周期的波形即可。 采样例图如下，红点表示采样点：任意顶点的相位会落在某两个采样点l, l+1之间，其z值可以通过对相应区间进行线性内插得到。 实现Gerstner Wave的任意相位的z值计算：1234567891011121314151617181920212223242526float gerstnerZ(float w, float h, float x) const&#123; const auto T = 2 * sampleX[sampleCount - 1]; x = fmodf(x * T / w, T); if (x &gt; T / 2) &#123; x = T - x; &#125; // search for xpos section int l = 0, r = sampleCount, m = (l + r) &gt;&gt; 1; while (r - l &gt; 1) &#123; if (sampleX[m] &gt; x) &#123; r = m; &#125; else &#123; l = m; &#125; m = (l + r) &gt;&gt; 1; &#125; // linear interpolation return h * (sampleY[l] + (x - sampleX[l]) / (sampleX[r] - sampleX[l]) * (sampleY[r] - sampleY[l]));&#125; 自适应细节的离线波形生成在离线生成波形时，我们希望在波形较尖锐处插入更多的采样点，而在波形较少处插入较少的采样点（如上图）。假设要插入采样点的个数为n，则采样区间的个数为m=n-1，使用等差数列自适应采样区间长度，有下式：$$S = \sum_{i=0}^{m-1} l_i = \pi$$Q=0时，Gerstner Wave为正弦波，令采样区间均匀，即：$$l_0 = \frac S m, d = 0$$Q=1时，假设最短的采样区间的长度为采样区间平均长度的1/3，有下式：$$l_0 = \frac S {3m}, d = \frac {4\pi} {3m(m-1)}$$对Q进行线性内插，得：$$l_0(Q) = l_0(1) Q + l_0(0) (1-Q) = \frac {\pi} m (1 - \frac 2 3 Q)$$$$d(Q) = d(1) Q + d(0) (1-Q) = \frac {4\pi} {3m(m-1)} Q$$ 上述算法的C++实现12345678910111213void setSample()&#123; // sharpness-precision auto adaption float start = M_PI * (1 - 2.f / 3.f * fSharpness) / (sampleCount - 1); float det = 4 * M_PI * fSharpness / (3.f * (sampleCount - 1) * (sampleCount - 2)); auto alpha = M_PI; for (int i = sampleCount - 1; i &gt;= 0; --i) &#123; sampleX[i] = alpha - fSharpness * sinf(alpha - M_PI); sampleY[i] = cosf(alpha - M_PI); alpha -= start; start += det; &#125;&#125; 法向量生成顶点的法向量可以通过相邻四个面的法向量取平均值获得，如图：$$\overrightarrow n = normalize(\overrightarrow {v_0} \times \overrightarrow {v_1} + \overrightarrow {v_1} \times \overrightarrow {v_2} + \overrightarrow {v_2} \times \overrightarrow {v_3} + \overrightarrow {v_3} \times \overrightarrow {v_0})$$ 也可对参数方程微分获得。 GPU版本上述实现只包含CPU版本，因为生成法向量需要旁边顶点的坐标。后续考虑会用微分+线性内插的方法生成法向量，这样就可以用vertex shader实现上述计算。 其他水面波形模拟方法shaderToy - Seascape，渲染效果非常棒，最近也有拜读这份代码的打算。 … 其他继承树： bee 我要变成小妹妹了&gt;w&lt;]]></content>
      <tags>
        <tag>GL</tag>
        <tag>water-rendering</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017.Nov]]></title>
    <url>%2F2017%2F11%2F29%2F2017-Nov%2F</url>
    <content type="text"><![CDATA[Nov降温啦&gt;_&lt; 月初用JS+遗传算法写了一个无向图的自动画图脚本，现阶段处于放置play状态 双十一买了一块茶轴的CrazyBoard我的第一块机械键盘（-500），手感一般，打字的时候有种泡水里的感觉。。。不过确实有提升手速，打串稳了不少，ACC略有提升。《并行程序设计导论》 考试周pass 考试周结束后画了很多，但是都不很满意，没放在p站上。线条和色彩还都控制得不是很到位对着《并行程序设计导论》把MPI的接口熟悉了一下 几天前tampermonkey初使用冬学期第一周，物联网课和学弟同班，成功组队 今天把数逻的VGA接口写完了，准备尽早弄完大作业，好腾出期末时间复习 osu!很久没刷pp了，神清气爽 近期计划 数逻大作业 看体系结构，编译优化 玩玩Intel Parallel Studio 学习一下C#的反射，看osu!framework的源代码]]></content>
      <tags>
        <tag>thoughts</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[blog加载速度优化]]></title>
    <url>%2F2017%2F11%2F09%2Fblog%E5%8A%A0%E8%BD%BD%E9%80%9F%E5%BA%A6%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[自开通blog后收到了数不胜数的 网页加载过慢 的投诉…自己试了试，在关掉cache的情况下达到了非常不能忍的加载速度，大概加载完要50多秒…在某些不加载完就不显示页面的浏览器上更让人抓狂（咳咳由于以上原因决定试着做点优化… 背景图其实背景图有点花。为了不喧宾夺主，已经被我打了个高斯模糊，但是裸加载的时候是从上到下加载的，看上去非常傻.. 缩图到了十分之一大小，感觉加载速度变快了哈哈.jpg反正都高斯模糊了 P.S.发现Edge不支持blur？！在论坛上看到这个bug在新版本中修复了，但是微软似乎还没给我推送这个更新…哈哈… 蜜汁脚本缩了图后加载速度从50s降到了大概35s，感觉没什么太大效果。用hexo s试了一下本地加载，发现一样慢…用chrome的waterfall检查，结果大概这样：emmmm…发现果然有迷之脚本Valine.min.js，产生了巨长的等待时间。仔细检查一下Request，结果发现源脚本被302重定向了..经过没过大脑的思考，我把源文件手动download并link了一下，大功告成！blog加载速度飞一样的提升.. 回头想想发现我好像没有引用Valine..123456# Valine.# You can get your appid and appkey from https://leancloud.cn# more info please open https://valine.js.orgvaline: enable: false # ... 确实没有。这就很有趣了… 在目录里搜索了一下valine，发现valine.swig里有个bug：好像没有swig高亮，用html将就一下了…1234567&lt;script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"&gt;&lt;/script&gt;&lt;script src="//unpkg.com/valine/dist/Valine.min.js"&gt;&lt;/script&gt;&#123;% if theme.valine.enable and theme.valine.appid and theme.valine.appkey %&#125; &lt;script type="text/javascript"&gt; // .... &lt;/script&gt;&#123;% endif %&#125; 问号.jpg这个引用居然在if外面？？ 最终修改：1234567&#123;% if theme.valine.enable and theme.valine.appid and theme.valine.appkey %&#125; &lt;script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"&gt;&lt;/script&gt; &lt;script src="//unpkg.com/valine/dist/Valine.min.js"&gt;&lt;/script&gt; &lt;script type="text/javascript"&gt; // .... &lt;/script&gt;&#123;% endif %&#125; CDN加速！试用了一下BootCDN，感觉效果很不错.. 总结无cache秒开..]]></content>
      <tags>
        <tag>thoughts</tag>
        <tag>blog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[koishi.top!]]></title>
    <url>%2F2017%2F11%2F05%2Fkoishi-top%2F</url>
    <content type="text"><![CDATA[打算给github page弄个域名，于是去万网逛了一圈发现了这个…恋恋世界第一未来十年也要做萝莉控啊QwQ]]></content>
      <tags>
        <tag>thoughts</tag>
        <tag>blog</tag>
        <tag>hello-world</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F404%2Findex.html</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2Fabout%2Fabout.css</url>
    <content type="text"><![CDATA[.koishi-block { width: 300px; }]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2Fabout%2Fabout.js</url>
    <content type="text"><![CDATA[// $(function(){ // $(".fold-button").click(function(){ // $(".fold-box").slideToggle("slow"); // $(".open").toggle(); // $(".close").toggle(); // }); // });]]></content>
  </entry>
  <entry>
    <title><![CDATA[about KoishiChan]]></title>
    <url>%2Fabout%2Findex.html</url>
    <content type="text"><![CDATA[KoishiChan不是很想写自我介绍，并且委屈地哭了起来.jpg 浙江大学·16级计科现浙江大学16级计算机系本科生之一。 github除了作业以外的一大半代码都在github上了，所以可能会显得比较杂乱还算比较绿，看得过去 常用语言 C++ Python Typescript Object Pascal pixiv咸鱼画手，主要发表草稿和涂鸦级别的作品，不过一直在坚持上色有奇怪倾向的同人插画 osu!2017.4 - ????.?咸鱼osu玩家，主要症状表现为 令人窒息的acc 压倒性的retry次数 pp虚高得很 skin koishi~(Enhanced) 2017/11/6 koishi~(Enhanced) 2017/11/22 koishi~(Enhanced) 2018/1/3 总结现阶段咸鱼一条，正在努力变得不那么咸中…]]></content>
  </entry>
  <entry>
    <title><![CDATA[tags]]></title>
    <url>%2Ftags%2Findex.html</url>
    <content type="text"></content>
  </entry>
</search>
